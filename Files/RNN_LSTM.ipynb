{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malaika-n/Capstone-Project/blob/main/RNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "datalore": {
          "node_id": "ieY7jm7UkEJKcNmkd22exy",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "lHSzKl9iVUXFUaBMJBsKX7"
          }
        },
        "id": "MD1dh2-eGrbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import ndarray"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "XB2Os4pmaa51hMkOmQTmzq",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "ckrFq9pI1tsQGVVFuO0wni"
          }
        },
        "id": "UKdjsVnTGrbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "JSGAcAKYToY6oTRIJ1L3Fc",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "VTpey4SMTz28vYcLWwww4O"
          }
        },
        "id": "RLEAnEJbGrbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')\n",
        "%matplotlib inline\n",
        "\n",
        "from copy import deepcopy\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-985925b8c214>:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-white')\n"
          ]
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "HHB9S5Jw2BNGvBzy2qymgC",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "dFoLslkJGxuPpGggUOHC6M"
          }
        },
        "id": "C0PxPzxnGrbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de8c0cff-aa20-43f5-f615-dff48f2f6990"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lincoln.utils.np_utils import assert_same_shape\n",
        "from scipy.special import logsumexp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3f3fd0d33eec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlincoln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_same_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lincoln'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "metadata": {
        "datalore": {
          "node_id": "BQp8FOBQdPj41bcqefASfm",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "G08C50BIyMxZL1DcatrcuN"
          }
        },
        "id": "YMIrZ7uFGrbK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "7e0a2ec1-0010-441f-8b76-e7ab18b191fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activations"
      ],
      "metadata": {
        "datalore": {
          "node_id": "laUhDCIcJxSlVuwhlnBQ5Y",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "dKc1ie2KFNMezCkO1xuLm9"
          }
        },
        "id": "vsfAxwBnGrbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x: ndarray):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def dsigmoid(x: ndarray):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "\n",
        "def tanh(x: ndarray):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def dtanh(x: ndarray):\n",
        "    return 1 - np.tanh(x) * np.tanh(x)\n",
        "\n",
        "\n",
        "def softmax(x, axis=None):\n",
        "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
        "\n",
        "\n",
        "def batch_softmax(input_array: ndarray):\n",
        "    out = []\n",
        "    for row in input_array:\n",
        "        out.append(softmax(row, axis=1))\n",
        "    return np.stack(out)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "u8fUHaks1Oql8dHDrsXDS3",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "TXnwem0q9WvTysNeOuSCSF"
          }
        },
        "id": "5YQGnyxHGrbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `RNNOptimizer`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "awheuZ1dvZh8wesafeqfbK",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "v9vtZJINeactA3kDVJGaOA"
          }
        },
        "id": "NWfgPGIVGrbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNOptimizer(object):\n",
        "    def __init__(self,\n",
        "                 lr: float = 0.01,\n",
        "                 gradient_clipping: bool = True) -> None:\n",
        "        self.lr = lr\n",
        "        self.gradient_clipping = gradient_clipping\n",
        "        self.first = True\n",
        "\n",
        "    def step(self) -> None:\n",
        "\n",
        "        for layer in self.model.layers:\n",
        "            for key in layer.params.keys():\n",
        "\n",
        "                if self.gradient_clipping:\n",
        "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
        "\n",
        "                self._update_rule(param=layer.params[key]['value'],\n",
        "                                  grad=layer.params[key]['deriv'])\n",
        "\n",
        "    def _update_rule(self, **kwargs) -> None:\n",
        "        raise NotImplementedError()"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "Plz6DXKnnuSdkkEl3s5yCk",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "4ghefMrK96NCnuEh63ejg5"
          }
        },
        "id": "gLtVRS3VGrbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `SGD` and `AdaGrad`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "TRSb9Ytm1UYdG7uI4Sycul",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "0DWf2TtzrLJBTeHfJzIeC7"
          }
        },
        "id": "dT5Jd31oGrbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(RNNOptimizer):\n",
        "    def __init__(self,\n",
        "                 lr: float = 0.01,\n",
        "                 gradient_clipping: bool = True) -> None:\n",
        "        super().__init__(lr, gradient_clipping)\n",
        "\n",
        "    def _update_rule(self, **kwargs) -> None:\n",
        "\n",
        "        update = self.lr*kwargs['grad']\n",
        "        kwargs['param'] -= update\n"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "H8Bb0NOVC3hWM1kbKRaHYH",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "gew4wZjPcYLT09QngyNVnb"
          }
        },
        "id": "JnHuYSkyGrbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad(RNNOptimizer):\n",
        "    def __init__(self,\n",
        "                 lr: float = 0.01,\n",
        "                gradient_clipping: bool = True) -> None:\n",
        "        super().__init__(lr, gradient_clipping)\n",
        "        self.eps = 1e-7\n",
        "\n",
        "    def step(self) -> None:\n",
        "        if self.first:\n",
        "            self.sum_squares = {}\n",
        "            for i, layer in enumerate(self.model.layers):\n",
        "                self.sum_squares[i] = {}\n",
        "                for key in layer.params.keys():\n",
        "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
        "\n",
        "            self.first = False\n",
        "\n",
        "        for i, layer in enumerate(self.model.layers):\n",
        "            for key in layer.params.keys():\n",
        "\n",
        "                if self.gradient_clipping:\n",
        "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
        "\n",
        "                self._update_rule(param=layer.params[key]['value'],\n",
        "                                  grad=layer.params[key]['deriv'],\n",
        "                                  sum_square=self.sum_squares[i][key])\n",
        "\n",
        "    def _update_rule(self, **kwargs) -> None:\n",
        "\n",
        "            # Update running sum of squares\n",
        "            kwargs['sum_square'] += (self.eps +\n",
        "                                     np.power(kwargs['grad'], 2))\n",
        "\n",
        "            # Scale learning rate by running sum of squareds=5\n",
        "            lr = np.divide(self.lr, np.sqrt(kwargs['sum_square']))\n",
        "\n",
        "            # Use this to update parameters\n",
        "            kwargs['param'] -= lr * kwargs['grad']"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "lv11Cp37cQdHI0CIsPGeNd",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "CyG99nTwplkvWXrcZjDmrX"
          }
        },
        "id": "or_w9r1bGrbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Loss`es"
      ],
      "metadata": {
        "datalore": {
          "node_id": "45ADwn2GSSS3sZeu2ILEjp",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "2rdOQUT9bcqjdSS2kmFU4j"
          }
        },
        "id": "Y2qqN_bTGrbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,\n",
        "                prediction: ndarray,\n",
        "                target: ndarray) -> float:\n",
        "\n",
        "        assert_same_shape(prediction, target)\n",
        "\n",
        "        self.prediction = prediction\n",
        "        self.target = target\n",
        "\n",
        "        self.output = self._output()\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self) -> ndarray:\n",
        "\n",
        "        self.input_grad = self._input_grad()\n",
        "\n",
        "        assert_same_shape(self.prediction, self.input_grad)\n",
        "\n",
        "        return self.input_grad\n",
        "\n",
        "    def _output(self) -> float:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _input_grad(self) -> ndarray:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class SoftmaxCrossEntropy(Loss):\n",
        "    def __init__(self, eps: float=1e-9) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.single_class = False\n",
        "\n",
        "    def _output(self) -> float:\n",
        "\n",
        "        out = []\n",
        "        for row in self.prediction:\n",
        "            out.append(softmax(row, axis=1))\n",
        "        softmax_preds = np.stack(out)\n",
        "\n",
        "        # clipping the softmax output to prevent numeric instability\n",
        "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
        "\n",
        "        # actual loss computation\n",
        "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
        "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
        "\n",
        "        return np.sum(softmax_cross_entropy_loss)\n",
        "\n",
        "    def _input_grad(self) -> np.ndarray:\n",
        "\n",
        "        return self.softmax_preds - self.target"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "XwRB0M5KcNGf0b4xJnldfo",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "8kDVJ52IiYvONisvyt8kd3"
          }
        },
        "id": "37RcZEgBGrbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs"
      ],
      "metadata": {
        "datalore": {
          "node_id": "VBruR5WllUBXiZf6lWyw5E",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "vXxOqNGHpkRs5In9wEP0TD"
          }
        },
        "id": "nHXLPriOGrbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `RNNNode`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "Me3ZgQTupwY5fNRWFo1tdw",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "sB38iZb7Pz5NgSlRkufy7H"
          }
        },
        "id": "l9Uf4lrBGrbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNNode(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,\n",
        "                x_in: ndarray,\n",
        "                H_in: ndarray,\n",
        "                params_dict: Dict[str, Dict[str, ndarray]]\n",
        "                ) -> Tuple[ndarray]:\n",
        "        '''\n",
        "        param x: numpy array of shape (batch_size, vocab_size)\n",
        "        param H_prev: numpy array of shape (batch_size, hidden_size)\n",
        "        return self.x_out: numpy array of shape (batch_size, vocab_size)\n",
        "        return self.H: numpy array of shape (batch_size, hidden_size)\n",
        "        '''\n",
        "        self.X_in = x_in\n",
        "        self.H_in = H_in\n",
        "\n",
        "        self.Z = np.column_stack((x_in, H_in))\n",
        "\n",
        "        self.H_int = np.dot(self.Z, params_dict['W_f']['value']) \\\n",
        "                                    + params_dict['B_f']['value']\n",
        "\n",
        "        self.H_out = tanh(self.H_int)\n",
        "\n",
        "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) \\\n",
        "                                        + params_dict['B_v']['value']\n",
        "\n",
        "        return self.X_out, self.H_out\n",
        "\n",
        "\n",
        "    def backward(self,\n",
        "                 X_out_grad: ndarray,\n",
        "                 H_out_grad: ndarray,\n",
        "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
        "        '''\n",
        "        param x_out_grad: numpy array of shape (batch_size, vocab_size)\n",
        "        param h_out_grad: numpy array of shape (batch_size, hidden_size)\n",
        "        param RNN_Params: RNN_Params object\n",
        "        return x_in_grad: numpy array of shape (batch_size, vocab_size)\n",
        "        return h_in_grad: numpy array of shape (batch_size, hidden_size)\n",
        "        '''\n",
        "\n",
        "        assert_same_shape(X_out_grad, self.X_out)\n",
        "        assert_same_shape(H_out_grad, self.H_out)\n",
        "\n",
        "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
        "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
        "\n",
        "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
        "        dh += H_out_grad\n",
        "\n",
        "        dH_int = dh * dtanh(self.H_int)\n",
        "\n",
        "        params_dict['B_f']['deriv'] += dH_int.sum(axis=0)\n",
        "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)\n",
        "\n",
        "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
        "\n",
        "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
        "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
        "\n",
        "        assert_same_shape(X_out_grad, self.X_out)\n",
        "        assert_same_shape(H_out_grad, self.H_out)\n",
        "\n",
        "        return X_in_grad, H_in_grad"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "EizSIzjj2xTxlaATFQHvjv",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "qbOaX4tC54UKv3g7h4qdE2"
          }
        },
        "id": "o7kazrwBGrbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `RNNLayer`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "FoH0IixdjaFGOALa5GtZ6Q",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "lHpXqxCH5fYOk8VQfdjiVn"
          }
        },
        "id": "fN-t6eYaGrbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLayer(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_size: int,\n",
        "                 output_size: int,\n",
        "                 weight_scale: float = None):\n",
        "        '''\n",
        "        param sequence_length: int - length of sequence being passed through the network\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
        "        param learning_rate: float - the learning rate\n",
        "        '''\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.weight_scale = weight_scale\n",
        "        self.start_H = np.zeros((1, hidden_size))\n",
        "        self.first = True\n",
        "\n",
        "\n",
        "    def _init_params(self,\n",
        "                     input_: ndarray):\n",
        "\n",
        "        self.vocab_size = input_.shape[2]\n",
        "\n",
        "        if not self.weight_scale:\n",
        "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
        "\n",
        "        self.params = {}\n",
        "        self.params['W_f'] = {}\n",
        "        self.params['B_f'] = {}\n",
        "        self.params['W_v'] = {}\n",
        "        self.params['B_v'] = {}\n",
        "\n",
        "        self.params['W_f']['value'] = np.random.normal(loc = 0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
        "        self.params['B_f']['value'] = np.random.normal(loc = 0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.hidden_size))\n",
        "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size, self.output_size))\n",
        "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.output_size))\n",
        "\n",
        "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
        "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
        "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
        "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
        "\n",
        "        self.cells = [RNNNode() for x in range(input_.shape[1])]\n",
        "\n",
        "\n",
        "    def _clear_gradients(self):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
        "\n",
        "\n",
        "    def forward(self, x_seq_in: ndarray):\n",
        "        '''\n",
        "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        return x_seq_out: numpy array of shape (batch_size, sequence_length, output_size)\n",
        "        '''\n",
        "        if self.first:\n",
        "            self._init_params(x_seq_in)\n",
        "            self.first=False\n",
        "\n",
        "        batch_size = x_seq_in.shape[0]\n",
        "\n",
        "        H_in = np.copy(self.start_H)\n",
        "\n",
        "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
        "\n",
        "        sequence_length = x_seq_in.shape[1]\n",
        "\n",
        "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "\n",
        "            x_in = x_seq_in[:, t, :]\n",
        "\n",
        "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
        "\n",
        "            x_seq_out[:, t, :] = y_out\n",
        "\n",
        "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
        "\n",
        "        return x_seq_out\n",
        "\n",
        "\n",
        "    def backward(self, x_seq_out_grad: ndarray):\n",
        "        '''\n",
        "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "        batch_size = x_seq_out_grad.shape[0]\n",
        "\n",
        "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        sequence_length = x_seq_out_grad.shape[1]\n",
        "\n",
        "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
        "\n",
        "        for t in reversed(range(sequence_length)):\n",
        "\n",
        "            x_out_grad = x_seq_out_grad[:, t, :]\n",
        "\n",
        "            grad_out, h_in_grad = \\\n",
        "                self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
        "\n",
        "            x_seq_in_grad[:, t, :] = grad_out\n",
        "\n",
        "        return x_seq_in_grad"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "ZXfoe50qRaeb8SVxK7sYUO",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "s5iuX4Nc41VQKWOonAFsnq"
          }
        },
        "id": "F7BNR_EcGrbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `RNNModel`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "BqQ6b5hnYUXfP2JgfEYMCV",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "Jj0qrJZoxXb6IEdfbBaSAw"
          }
        },
        "id": "suuYEb-gGrbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(object):\n",
        "    '''\n",
        "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 layers: List[RNNLayer],\n",
        "                 sequence_length: int,\n",
        "                 vocab_size: int,\n",
        "                 loss: Loss):\n",
        "        '''\n",
        "        param num_layers: int - the number of layers in the network\n",
        "        param sequence_length: int - length of sequence being passed through the network\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
        "        '''\n",
        "        self.layers = layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.loss = loss\n",
        "        for layer in self.layers:\n",
        "            setattr(layer, 'sequence_length', sequence_length)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x_batch: ndarray):\n",
        "        '''\n",
        "        param inputs: list of integers - a list of indices of characters being passed in as the\n",
        "        input sequence of the network.\n",
        "        returns x_batch_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "\n",
        "        for layer in self.layers:\n",
        "\n",
        "            x_batch = layer.forward(x_batch)\n",
        "\n",
        "        return x_batch\n",
        "\n",
        "    def backward(self,\n",
        "                 loss_grad: ndarray):\n",
        "        '''\n",
        "        param loss_grad: numpy array with shape (batch_size, sequence_length, vocab_size)\n",
        "        returns loss: float, representing mean squared error loss\n",
        "        '''\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "\n",
        "            loss_grad = layer.backward(loss_grad)\n",
        "\n",
        "        return loss_grad\n",
        "\n",
        "    def single_step(self,\n",
        "                    x_batch: ndarray,\n",
        "                    y_batch: ndarray):\n",
        "        '''\n",
        "        The step that does it all:\n",
        "        1. Forward pass & softmax\n",
        "        2. Compute loss and loss gradient\n",
        "        3. Backward pass\n",
        "        4. Update parameters\n",
        "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
        "        the network\n",
        "        param targets: array of length sequence_length that represents the character indices of the targets\n",
        "        of the network\n",
        "        return loss\n",
        "        '''\n",
        "\n",
        "        x_batch_out = self.forward(x_batch)\n",
        "\n",
        "        loss = self.loss.forward(x_batch_out, y_batch)\n",
        "\n",
        "        loss_grad = self.loss.backward()\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer._clear_gradients()\n",
        "\n",
        "        self.backward(loss_grad)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "jwNgoTL5JCor7FdKXYZQ7G",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "V2YzZoLFUQjorlooYIyC2o"
          }
        },
        "id": "jBzzvoAKGrbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `RNNTrainer`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "UlIJXQtSR4tuRlZKZdxeAQ",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "Yrzv8EbXI2qH2VTqLAhW66"
          }
        },
        "id": "NiHXwS6iGrbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNTrainer:\n",
        "    '''\n",
        "    Takes in a text file and a model, and starts generating characters.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 text_file: str,\n",
        "                 model: RNNModel,\n",
        "                 optim: RNNOptimizer,\n",
        "                 batch_size: int = 32):\n",
        "        self.data = open(text_file, 'r').read()\n",
        "        self.model = model\n",
        "        self.chars = list(set(self.data))\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
        "        self.sequence_length = self.model.sequence_length\n",
        "        self.batch_size = batch_size\n",
        "        self.optim = optim\n",
        "        setattr(self.optim, 'model', self.model)\n",
        "\n",
        "\n",
        "    def _generate_inputs_targets(self,\n",
        "                                 start_pos: int):\n",
        "\n",
        "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
        "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "\n",
        "            inputs_indices[i, :] = np.array([self.char_to_idx[ch]\n",
        "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
        "            targets_indices[i, :] = np.array([self.char_to_idx[ch]\n",
        "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
        "\n",
        "        return inputs_indices, targets_indices\n",
        "\n",
        "\n",
        "    def _generate_one_hot_array(self,\n",
        "                                indices: ndarray):\n",
        "        '''\n",
        "        param indices: numpy array of shape (batch_size, sequence_length)\n",
        "        return batch - numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "        batch = []\n",
        "        for seq in indices:\n",
        "\n",
        "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
        "\n",
        "            for i in range(self.sequence_length):\n",
        "                one_hot_sequence[i, seq[i]] = 1.0\n",
        "\n",
        "            batch.append(one_hot_sequence)\n",
        "\n",
        "        return np.stack(batch)\n",
        "\n",
        "\n",
        "    def sample_output(self,\n",
        "                      input_char: int,\n",
        "                      sample_length: int):\n",
        "        '''\n",
        "        Generates a sample output using the current trained model, one character at a time.\n",
        "        param input_char: int - index of the character to use to start generating a sequence\n",
        "        param sample_length: int - the length of the sample output to generate\n",
        "        return txt: string - a string of length sample_length representing the sample output\n",
        "        '''\n",
        "        indices = []\n",
        "\n",
        "        sample_model = deepcopy(self.model)\n",
        "\n",
        "        for i in range(sample_length):\n",
        "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
        "\n",
        "            input_char_batch[0, 0, input_char] = 1.0\n",
        "\n",
        "            x_batch_out = sample_model.forward(input_char_batch)\n",
        "\n",
        "            x_softmax = batch_softmax(x_batch_out)\n",
        "\n",
        "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
        "\n",
        "            indices.append(input_char)\n",
        "\n",
        "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
        "        return txt\n",
        "\n",
        "    def train(self,\n",
        "              num_iterations: int,\n",
        "              sample_every: int=100):\n",
        "        '''\n",
        "        Trains the \"character generator\" for a number of iterations.\n",
        "        Each \"iteration\" feeds a batch size of 1 through the neural network.\n",
        "        Continues until num_iterations is reached. Displays sample text generated using the latest version.\n",
        "        '''\n",
        "        plot_iter = np.zeros((0))\n",
        "        plot_loss = np.zeros((0))\n",
        "\n",
        "        num_iter = 0\n",
        "        start_pos = 0\n",
        "\n",
        "        moving_average = deque(maxlen=100)\n",
        "        while num_iter < num_iterations:\n",
        "\n",
        "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
        "                start_pos = 0\n",
        "\n",
        "            ## Update the model\n",
        "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
        "\n",
        "            inputs_batch, targets_batch = \\\n",
        "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
        "\n",
        "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
        "            self.optim.step()\n",
        "\n",
        "            moving_average.append(loss)\n",
        "            ma_loss = np.mean(moving_average)\n",
        "\n",
        "            start_pos += self.batch_size\n",
        "\n",
        "            plot_iter = np.append(plot_iter, [num_iter])\n",
        "            plot_loss = np.append(plot_loss, [ma_loss])\n",
        "\n",
        "            if num_iter % 100 == 0:\n",
        "                plt.plot(plot_iter, plot_loss)\n",
        "                display.clear_output(wait=True)\n",
        "                plt.show()\n",
        "\n",
        "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]],\n",
        "                                                 200)\n",
        "                print(sample_text)\n",
        "\n",
        "            num_iter += 1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "Hi0c84GNuq2mc9YE5mzSeO",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "BdB1ZATJGPrmnvFSQ641Mq"
          }
        },
        "id": "8wrwgZRmGrbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [RNNLayer(hidden_size=256, output_size=62)]\n",
        "mod = RNNModel(layers=layers,\n",
        "               vocab_size=62, sequence_length=10,\n",
        "               loss=SoftmaxCrossEntropy())\n",
        "optim = SGD(lr=0.001, gradient_clipping=True)\n",
        "trainer = RNNTrainer('input.txt', mod, optim)\n",
        "trainer.train(1000, sample_every=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "9JrbbPQFSwQL88UtG26cb8",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "JHpFFyxRJ0DuSUFVCNSnxq"
          }
        },
        "id": "h1Vqj1L_GrbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With RNN cells, this gets stuck in a local max. Let's try `LSTM`s."
      ],
      "metadata": {
        "datalore": {
          "node_id": "Gu5crGVFiwExTtF3UrmGa8",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "JBakNfvosaCZNyvpjcwFlN"
          }
        },
        "id": "13YpfuJKGrbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTMs"
      ],
      "metadata": {
        "datalore": {
          "node_id": "hpEuVh6Qm7cGAXU3GKjfNE",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "cxNcVBCDAku9zgU1783VvL"
          }
        },
        "id": "6yHDf57zGrbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `LSTMNode`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "NdkXLt1hRt14LxnasltMZ0",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "JphkIIMV3ATlo6Rq53ZogP"
          }
        },
        "id": "HR5ksAq4GrbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMNode:\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def forward(self,\n",
        "                X_in: ndarray,\n",
        "                H_in: ndarray,\n",
        "                C_in: ndarray,\n",
        "                params_dict: Dict[str, Dict[str, ndarray]]):\n",
        "        '''\n",
        "        param X_in: numpy array of shape (batch_size, vocab_size)\n",
        "        param H_in: numpy array of shape (batch_size, hidden_size)\n",
        "        param C_in: numpy array of shape (batch_size, hidden_size)\n",
        "        return self.X_out: numpy array of shape (batch_size, output_size)\n",
        "        return self.H: numpy array of shape (batch_size, hidden_size)\n",
        "        return self.C: numpy array of shape (batch_size, hidden_size)\n",
        "        '''\n",
        "        self.X_in = X_in\n",
        "        self.C_in = C_in\n",
        "\n",
        "        self.Z = np.column_stack((X_in, H_in))\n",
        "\n",
        "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
        "        self.f = sigmoid(self.f_int)\n",
        "\n",
        "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
        "        self.i = sigmoid(self.i_int)\n",
        "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
        "        self.C_bar = tanh(self.C_bar_int)\n",
        "\n",
        "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
        "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
        "        self.o = sigmoid(self.o_int)\n",
        "        self.H_out = self.o * tanh(self.C_out)\n",
        "\n",
        "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
        "\n",
        "        return self.X_out, self.H_out, self.C_out\n",
        "\n",
        "\n",
        "    def backward(self,\n",
        "                 X_out_grad: ndarray,\n",
        "                 H_out_grad: ndarray,\n",
        "                 C_out_grad: ndarray,\n",
        "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
        "        '''\n",
        "        param loss_grad: numpy array of shape (1, vocab_size)\n",
        "        param dh_next: numpy array of shape (1, hidden_size)\n",
        "        param dC_next: numpy array of shape (1, hidden_size)\n",
        "        param LSTM_Params: LSTM_Params object\n",
        "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
        "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
        "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
        "        '''\n",
        "\n",
        "        assert_same_shape(X_out_grad, self.X_out)\n",
        "        assert_same_shape(H_out_grad, self.H_out)\n",
        "        assert_same_shape(C_out_grad, self.C_out)\n",
        "\n",
        "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
        "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
        "\n",
        "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
        "        dh_out += H_out_grad\n",
        "\n",
        "        do = dh_out * tanh(self.C_out)\n",
        "        do_int = dsigmoid(self.o_int) * do\n",
        "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
        "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
        "\n",
        "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
        "        dC_out += C_out_grad\n",
        "        dC_bar = dC_out * self.i\n",
        "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
        "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
        "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
        "\n",
        "        di = dC_out * self.C_bar\n",
        "        di_int = dsigmoid(self.i_int) * di\n",
        "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
        "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
        "\n",
        "        df = dC_out * self.C_in\n",
        "        df_int = dsigmoid(self.f_int) * df\n",
        "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
        "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
        "\n",
        "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)\n",
        "             + np.dot(di_int, params_dict['W_i']['value'].T)\n",
        "             + np.dot(dC_bar_int, params_dict['W_c']['value'].T)\n",
        "             + np.dot(do_int, params_dict['W_o']['value'].T))\n",
        "\n",
        "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
        "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
        "        dC_prev = self.f * dC_out\n",
        "\n",
        "        return dx_prev, dH_prev, dC_prev"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "rjK43GJb6Gvq2gqWCpSv4g",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "8l4epR9jn54OFxbtA7Szpw"
          }
        },
        "id": "7QCrmSFeGrbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `LSTMLayer`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "s308jmENH2maXXsb36tZ0u",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "1SO6nbzdpANeQCdbxzZzY5"
          }
        },
        "id": "cdMgensQGrbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLayer:\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_size: int,\n",
        "                 output_size: int,\n",
        "                 weight_scale: float = 0.01):\n",
        "        '''\n",
        "        param sequence_length: int - length of sequence being passed through the network\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
        "        param learning_rate: float - the learning rate\n",
        "        '''\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.weight_scale = weight_scale\n",
        "        self.start_H = np.zeros((1, hidden_size))\n",
        "        self.start_C = np.zeros((1, hidden_size))\n",
        "        self.first = True\n",
        "\n",
        "\n",
        "    def _init_params(self,\n",
        "                     input_: ndarray):\n",
        "\n",
        "        self.vocab_size = input_.shape[2]\n",
        "\n",
        "        self.params = {}\n",
        "        self.params['W_f'] = {}\n",
        "        self.params['B_f'] = {}\n",
        "        self.params['W_i'] = {}\n",
        "        self.params['B_i'] = {}\n",
        "        self.params['W_c'] = {}\n",
        "        self.params['B_c'] = {}\n",
        "        self.params['W_o'] = {}\n",
        "        self.params['B_o'] = {}\n",
        "        self.params['W_v'] = {}\n",
        "        self.params['B_v'] = {}\n",
        "\n",
        "        self.params['W_f']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size =(self.hidden_size + self.vocab_size, self.hidden_size))\n",
        "        self.params['B_f']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size=(1, self.hidden_size))\n",
        "        self.params['W_i']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
        "        self.params['B_i']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.hidden_size))\n",
        "        self.params['W_c']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
        "        self.params['B_c']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.hidden_size))\n",
        "        self.params['W_o']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
        "        self.params['B_o']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.hidden_size))\n",
        "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(self.hidden_size, self.output_size))\n",
        "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.output_size))\n",
        "\n",
        "        for key in self.params.keys():\n",
        "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
        "\n",
        "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
        "\n",
        "\n",
        "    def _clear_gradients(self):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
        "\n",
        "\n",
        "    def forward(self, x_seq_in: ndarray):\n",
        "        '''\n",
        "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        return x_seq_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "        if self.first:\n",
        "            self._init_params(x_seq_in)\n",
        "            self.first=False\n",
        "\n",
        "        batch_size = x_seq_in.shape[0]\n",
        "\n",
        "        H_in = np.copy(self.start_H)\n",
        "        C_in = np.copy(self.start_C)\n",
        "\n",
        "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
        "        C_in = np.repeat(C_in, batch_size, axis=0)\n",
        "\n",
        "        sequence_length = x_seq_in.shape[1]\n",
        "\n",
        "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "\n",
        "            x_in = x_seq_in[:, t, :]\n",
        "\n",
        "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
        "\n",
        "            x_seq_out[:, t, :] = y_out\n",
        "\n",
        "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
        "        self.start_C = C_in.mean(axis=0, keepdims=True)\n",
        "\n",
        "        return x_seq_out\n",
        "\n",
        "\n",
        "    def backward(self, x_seq_out_grad: ndarray):\n",
        "        '''\n",
        "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "\n",
        "        batch_size = x_seq_out_grad.shape[0]\n",
        "\n",
        "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
        "        c_in_grad = np.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        num_chars = x_seq_out_grad.shape[1]\n",
        "\n",
        "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
        "\n",
        "        for t in reversed(range(num_chars)):\n",
        "\n",
        "            x_out_grad = x_seq_out_grad[:, t, :]\n",
        "\n",
        "            grad_out, h_in_grad, c_in_grad = \\\n",
        "                self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
        "\n",
        "            x_seq_in_grad[:, t, :] = grad_out\n",
        "\n",
        "        return x_seq_in_grad"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "lsVMLVJSQPfKTeYVrSuTke",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "utNSXvy8VsSTB1GoG2NAx4"
          }
        },
        "id": "fCtyXMYuGrbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `LSTMModel`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "AtJFKK0rw1zgyevwAV3aSs",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "lpI7zQJeplKLHj4MwbiYw1"
          }
        },
        "id": "ZYwr8QKAGrbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(object):\n",
        "    '''\n",
        "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 layers: List[LSTMLayer],\n",
        "                 sequence_length: int,\n",
        "                 vocab_size: int,\n",
        "                 hidden_size: int,\n",
        "                 loss: Loss):\n",
        "        '''\n",
        "        param num_layers: int - the number of layers in the network\n",
        "        param sequence_length: int - length of sequence being passed through the network\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
        "        '''\n",
        "        self.layers = layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.loss = loss\n",
        "        for layer in self.layers:\n",
        "            setattr(layer, 'sequence_length', sequence_length)\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                x_batch: ndarray):\n",
        "        '''\n",
        "        param inputs: list of integers - a list of indices of characters being passed in as the\n",
        "        input sequence of the network.\n",
        "        returns x_batch_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "\n",
        "        for layer in self.layers:\n",
        "\n",
        "            x_batch = layer.forward(x_batch)\n",
        "\n",
        "        return x_batch\n",
        "\n",
        "    def backward(self,\n",
        "                 loss_grad: ndarray):\n",
        "        '''\n",
        "        param loss_grad: numpy array with shape (batch_size, sequence_length, vocab_size)\n",
        "        returns loss: float, representing mean squared error loss\n",
        "        '''\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "\n",
        "            loss_grad = layer.backward(loss_grad)\n",
        "\n",
        "        return loss_grad\n",
        "\n",
        "    def single_step(self,\n",
        "                    x_batch: ndarray,\n",
        "                    y_batch: ndarray):\n",
        "        '''\n",
        "        The step that does it all:\n",
        "        1. Forward pass & softmax\n",
        "        2. Compute loss and loss gradient\n",
        "        3. Backward pass\n",
        "        4. Update parameters\n",
        "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
        "        the network\n",
        "        param targets: array of length sequence_length that represents the character indices of the targets\n",
        "        of the network\n",
        "        return loss\n",
        "        '''\n",
        "\n",
        "        x_batch_out = self.forward(x_batch)\n",
        "\n",
        "        loss = self.loss.forward(x_batch_out, y_batch)\n",
        "\n",
        "        loss_grad = self.loss.backward()\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer._clear_gradients()\n",
        "\n",
        "        self.backward(loss_grad)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "KubZJlqsf8TFUjQ87LpJm3",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "YuhxUFFP8OemIiGtLIn4tf"
          }
        },
        "id": "Vb8Rw1YBGrbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRUs"
      ],
      "metadata": {
        "datalore": {
          "node_id": "Q8O7BvtIcXFDgF4mFKdDNa",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "EeaZ2c7ldIUyOMEaC9fP7W"
          }
        },
        "id": "d51s3QnkGrbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `GRUNode`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "wDqsX9HZ6mCwl0ZDbW7AIw",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "Jer5Bl6KSN9PzTmvZ2S6Pu"
          }
        },
        "id": "uCp44HmwGrbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUNode(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def forward(self,\n",
        "                X_in: ndarray,\n",
        "                H_in: ndarray,\n",
        "                params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
        "        '''\n",
        "        param X_in: numpy array of shape (batch_size, vocab_size)\n",
        "        param H_in: numpy array of shape (batch_size, hidden_size)\n",
        "        return self.X_out: numpy array of shape (batch_size, vocab_size)\n",
        "        return self.H_out: numpy array of shape (batch_size, hidden_size)\n",
        "        '''\n",
        "        self.X_in = X_in\n",
        "        self.H_in = H_in\n",
        "\n",
        "        # reset gate\n",
        "        self.X_r = np.dot(X_in, params_dict['W_xr']['value'])\n",
        "        self.H_r = np.dot(H_in, params_dict['W_hr']['value'])\n",
        "\n",
        "        # update gate\n",
        "        self.X_u = np.dot(X_in, params_dict['W_xu']['value'])\n",
        "        self.H_u = np.dot(H_in, params_dict['W_hu']['value'])\n",
        "\n",
        "        # gates\n",
        "        self.r_int = self.X_r + self.H_r + params_dict['B_r']['value']\n",
        "        self.r = sigmoid(self.r_int)\n",
        "\n",
        "        self.u_int = self.X_r + self.H_r + params_dict['B_u']['value']\n",
        "        self.u = sigmoid(self.u_int)\n",
        "\n",
        "        # new state\n",
        "        self.h_reset = self.r * H_in\n",
        "        self.X_h = np.dot(X_in, params_dict['W_xh']['value'])\n",
        "        self.H_h = np.dot(self.h_reset, params_dict['W_hh']['value'])\n",
        "        self.h_bar_int = self.X_h + self.H_h + params_dict['B_h']['value']\n",
        "        self.h_bar = tanh(self.h_bar_int)\n",
        "\n",
        "        self.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar\n",
        "\n",
        "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
        "\n",
        "        return self.X_out, self.H_out\n",
        "\n",
        "\n",
        "    def backward(self,\n",
        "                 X_out_grad: ndarray,\n",
        "                 H_out_grad: ndarray,\n",
        "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
        "\n",
        "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
        "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
        "\n",
        "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
        "        dh_out += H_out_grad\n",
        "\n",
        "        du = self.H_in * H_out_grad - self.h_bar * H_out_grad\n",
        "        dh_bar = (1 - self.u) * H_out_grad\n",
        "\n",
        "        dh_bar_int = dh_bar * dtanh(self.h_bar_int)\n",
        "        params_dict['B_h']['deriv'] += dh_bar_int.sum(axis=0)\n",
        "        params_dict['W_xh']['deriv'] += np.dot(self.X_in.T, dh_bar_int)\n",
        "\n",
        "        dX_in = np.dot(dh_bar_int, params_dict['W_xh']['value'].T)\n",
        "\n",
        "        params_dict['W_hh']['deriv'] += np.dot(self.h_reset.T, dh_bar_int)\n",
        "        dh_reset = np.dot(dh_bar_int, params_dict['W_hh']['value'].T)\n",
        "\n",
        "        dr = dh_reset * self.H_in\n",
        "        dH_in = dh_reset * self.r\n",
        "\n",
        "        # update branch\n",
        "        du_int = dsigmoid(self.u_int) * du\n",
        "        params_dict['B_u']['deriv'] += du_int.sum(axis=0)\n",
        "\n",
        "        dX_in += np.dot(du_int, params_dict['W_xu']['value'].T)\n",
        "        params_dict['W_xu']['deriv'] += np.dot(self.X_in.T, du_int)\n",
        "\n",
        "        dH_in += np.dot(du_int, params_dict['W_hu']['value'].T)\n",
        "        params_dict['W_hu']['deriv'] += np.dot(self.H_in.T, du_int)\n",
        "\n",
        "        # reset branch\n",
        "        dr_int = dsigmoid(self.r_int) * dr\n",
        "        params_dict['B_r']['deriv'] += dr_int.sum(axis=0)\n",
        "\n",
        "        dX_in += np.dot(dr_int, params_dict['W_xr']['value'].T)\n",
        "        params_dict['W_xr']['deriv'] += np.dot(self.X_in.T, dr_int)\n",
        "\n",
        "        dH_in += np.dot(dr_int, params_dict['W_hr']['value'].T)\n",
        "        params_dict['W_hr']['deriv'] += np.dot(self.H_in.T, dr_int)\n",
        "\n",
        "        return dX_in, dH_in"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "V0mvkehWA86SiEVYnRf4j5",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "9KYFy42rSqJRWGtlA9oKCm"
          }
        },
        "id": "EVGo4l_9GrbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `GRULayer`"
      ],
      "metadata": {
        "datalore": {
          "node_id": "ajzR0I9rOJioNoi0pZmEOK",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "5u1jVAxSt3m7nJKCH33Hpd"
          }
        },
        "id": "PGBHUQIjGrbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRULayer(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 hidden_size: int,\n",
        "                 output_size: int,\n",
        "                 weight_scale: float = 0.01):\n",
        "        '''\n",
        "        param sequence_length: int - length of sequence being passed through the network\n",
        "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
        "        character.\n",
        "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
        "        param learning_rate: float - the learning rate\n",
        "        '''\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.weight_scale = weight_scale\n",
        "        self.start_H = np.zeros((1, hidden_size))\n",
        "        self.first = True\n",
        "\n",
        "\n",
        "    def _init_params(self,\n",
        "                     input_: ndarray):\n",
        "\n",
        "        self.vocab_size = input_.shape[2]\n",
        "\n",
        "        self.params = {}\n",
        "        self.params['W_xr'] = {}\n",
        "        self.params['W_hr'] = {}\n",
        "        self.params['B_r'] = {}\n",
        "        self.params['W_xu'] = {}\n",
        "        self.params['W_hu'] = {}\n",
        "        self.params['B_u'] = {}\n",
        "        self.params['W_xh'] = {}\n",
        "        self.params['W_hh'] = {}\n",
        "        self.params['B_h'] = {}\n",
        "        self.params['W_v'] = {}\n",
        "        self.params['B_v'] = {}\n",
        "\n",
        "        self.params['W_xr']['value'] = np.random.normal(loc=0.0,\n",
        "                                                        scale=self.weight_scale,\n",
        "                                                        size=(self.vocab_size, self.hidden_size))\n",
        "        self.params['W_hr']['value'] = np.random.normal(loc=0.0,\n",
        "                                                        scale=self.weight_scale,\n",
        "                                                        size=(self.hidden_size, self.hidden_size))\n",
        "        self.params['B_r']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size=(1, self.hidden_size))\n",
        "        self.params['W_xu']['value'] = np.random.normal(loc=0.0,\n",
        "                                                        scale=self.weight_scale,\n",
        "                                                        size=(self.vocab_size, self.hidden_size))\n",
        "        self.params['W_hu']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size=(self.hidden_size, self.hidden_size))\n",
        "        self.params['B_u']['value'] = np.random.normal(loc=0.0,\n",
        "                                                      scale=self.weight_scale,\n",
        "                                                      size=(1, self.hidden_size))\n",
        "        self.params['W_xh']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size=(self.vocab_size, self.hidden_size))\n",
        "        self.params['W_hh']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=self.weight_scale,\n",
        "                                                       size=(self.hidden_size, self.hidden_size))\n",
        "        self.params['B_h']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=1.0,\n",
        "                                                       size=(1, self.hidden_size))\n",
        "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=1.0,\n",
        "                                                       size=(self.hidden_size, self.output_size))\n",
        "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
        "                                                       scale=1.0,\n",
        "                                                       size=(1, self.output_size))\n",
        "\n",
        "        for key in self.params.keys():\n",
        "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
        "\n",
        "        self.cells = [GRUNode() for x in range(input_.shape[1])]\n",
        "\n",
        "\n",
        "    def _clear_gradients(self):\n",
        "        for key in self.params.keys():\n",
        "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
        "\n",
        "\n",
        "    def forward(self, x_seq_in: ndarray):\n",
        "        '''\n",
        "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        return x_seq_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "        if self.first:\n",
        "            self._init_params(x_seq_in)\n",
        "            self.first=False\n",
        "\n",
        "        batch_size = x_seq_in.shape[0]\n",
        "\n",
        "        H_in = np.copy(self.start_H)\n",
        "\n",
        "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
        "\n",
        "        sequence_length = x_seq_in.shape[1]\n",
        "\n",
        "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "\n",
        "            x_in = x_seq_in[:, t, :]\n",
        "\n",
        "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
        "\n",
        "            x_seq_out[:, t, :] = y_out\n",
        "\n",
        "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
        "\n",
        "        return x_seq_out\n",
        "\n",
        "\n",
        "    def backward(self, x_seq_out_grad: ndarray):\n",
        "        '''\n",
        "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
        "        '''\n",
        "\n",
        "        batch_size = x_seq_out_grad.shape[0]\n",
        "\n",
        "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "        num_chars = x_seq_out_grad.shape[1]\n",
        "\n",
        "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
        "\n",
        "        for t in reversed(range(num_chars)):\n",
        "\n",
        "            x_out_grad = x_seq_out_grad[:, t, :]\n",
        "\n",
        "            grad_out, h_in_grad = \\\n",
        "                self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
        "\n",
        "            x_seq_in_grad[:, t, :] = grad_out\n",
        "\n",
        "        return x_seq_in_grad"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "K54nXMuHtLLwiG5ffkDDnR",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "uWZn91ldgse212mW0qYcDv"
          }
        },
        "id": "9bSMsVmVGrbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "datalore": {
          "node_id": "dtwldExbJAM9WNgMI54E6q",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "CHOr47gn0rJLS18ynXt50d"
          }
        },
        "id": "u7dIIxG4GrbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single LSTM layer"
      ],
      "metadata": {
        "datalore": {
          "node_id": "TD4yY9n5B17YVCiZTVO0FC",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "HMgCmQfnlkb4B3ark3DyCO"
          }
        },
        "id": "-MxuZvD0GrbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers1 = [LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
        "mod = RNNModel(layers=layers1,\n",
        "               vocab_size=62, sequence_length=25,\n",
        "               loss=SoftmaxCrossEntropy())\n",
        "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
        "trainer = RNNTrainer('input.txt', mod, optim, batch_size=3)\n",
        "trainer.train(1000, sample_every=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "ePZ8oRZjcWpyL0GhpriS63",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "oZkhiakEHX6AFeoQID1SyA"
          }
        },
        "id": "Q5pZ-mqRGrbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Three variants of multiple layers:"
      ],
      "metadata": {
        "datalore": {
          "node_id": "8Lxy475TIoVrW2oyvB2VQi",
          "type": "MD",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "bVu5FsBLa6iBx2HZDSVheP"
          }
        },
        "id": "_vV4uqe3GrbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers2 = [RNNLayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
        "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
        "mod = RNNModel(layers=layers2,\n",
        "               vocab_size=62, sequence_length=25,\n",
        "               loss=SoftmaxCrossEntropy())\n",
        "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
        "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
        "trainer.train(2000, sample_every=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "Y9PTsZ8YFDhJPXd8y6L6aB",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "2WnhOUkndXqwGT7X0ApNJJ"
          }
        },
        "id": "-N8-tploGrbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers2 = [LSTMLayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
        "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
        "mod = RNNModel(layers=layers2,\n",
        "               vocab_size=62, sequence_length=25,\n",
        "               loss=SoftmaxCrossEntropy())\n",
        "optim = SGD(lr=0.01, gradient_clipping=True)\n",
        "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
        "trainer.train(2000, sample_every=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "G7lIMo0cYIoPBTOrr3h8GI",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "5okNyj57n6V4ZiZCVXT1Ys"
          }
        },
        "id": "Sbjq6a5QGrbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers3 = [GRULayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
        "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
        "mod = RNNModel(layers=layers3,\n",
        "               vocab_size=62, sequence_length=25,\n",
        "               loss=SoftmaxCrossEntropy())\n",
        "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
        "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
        "trainer.train(2000, sample_every=100)"
      ],
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "datalore": {
          "node_id": "oUjryQ8k8wuQWdodstdDNu",
          "type": "CODE",
          "hide_input_from_viewers": false,
          "hide_output_from_viewers": false,
          "report_properties": {
            "rowId": "ieeHLzTfhEQqLLtQlLQtcA"
          }
        },
        "id": "iVODlc25GrbO"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python"
    },
    "datalore": {
      "computation_mode": "JUPYTER",
      "package_manager": "pip",
      "base_environment": "default",
      "packages": [],
      "report_row_ids": [
        "PCDRzEDUn4EVseb21IJxuu",
        "lHSzKl9iVUXFUaBMJBsKX7",
        "ckrFq9pI1tsQGVVFuO0wni",
        "VTpey4SMTz28vYcLWwww4O",
        "dFoLslkJGxuPpGggUOHC6M",
        "G08C50BIyMxZL1DcatrcuN",
        "dKc1ie2KFNMezCkO1xuLm9",
        "TXnwem0q9WvTysNeOuSCSF",
        "v9vtZJINeactA3kDVJGaOA",
        "4ghefMrK96NCnuEh63ejg5",
        "0DWf2TtzrLJBTeHfJzIeC7",
        "gew4wZjPcYLT09QngyNVnb",
        "CyG99nTwplkvWXrcZjDmrX",
        "2rdOQUT9bcqjdSS2kmFU4j",
        "8kDVJ52IiYvONisvyt8kd3",
        "vXxOqNGHpkRs5In9wEP0TD",
        "sB38iZb7Pz5NgSlRkufy7H",
        "qbOaX4tC54UKv3g7h4qdE2",
        "lHpXqxCH5fYOk8VQfdjiVn",
        "s5iuX4Nc41VQKWOonAFsnq",
        "Jj0qrJZoxXb6IEdfbBaSAw",
        "V2YzZoLFUQjorlooYIyC2o",
        "Yrzv8EbXI2qH2VTqLAhW66",
        "BdB1ZATJGPrmnvFSQ641Mq",
        "JHpFFyxRJ0DuSUFVCNSnxq",
        "JBakNfvosaCZNyvpjcwFlN",
        "cxNcVBCDAku9zgU1783VvL",
        "JphkIIMV3ATlo6Rq53ZogP",
        "8l4epR9jn54OFxbtA7Szpw",
        "1SO6nbzdpANeQCdbxzZzY5",
        "utNSXvy8VsSTB1GoG2NAx4",
        "lpI7zQJeplKLHj4MwbiYw1",
        "YuhxUFFP8OemIiGtLIn4tf",
        "EeaZ2c7ldIUyOMEaC9fP7W",
        "Jer5Bl6KSN9PzTmvZ2S6Pu",
        "9KYFy42rSqJRWGtlA9oKCm",
        "5u1jVAxSt3m7nJKCH33Hpd",
        "uWZn91ldgse212mW0qYcDv",
        "CHOr47gn0rJLS18ynXt50d",
        "HMgCmQfnlkb4B3ark3DyCO",
        "oZkhiakEHX6AFeoQID1SyA",
        "bVu5FsBLa6iBx2HZDSVheP",
        "2WnhOUkndXqwGT7X0ApNJJ",
        "5okNyj57n6V4ZiZCVXT1Ys",
        "ieeHLzTfhEQqLLtQlLQtcA"
      ],
      "version": 3
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}